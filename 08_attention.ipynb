{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "import torch\n",
    "from torch import nn\n",
    "import math\n",
    "from bertviz.transformers_neuron_view import BertModel, BertConfig\n",
    "from transformers import BertTokenizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. model config and load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 433/433 [00:00<?, ?B/s]\n",
      "100%|██████████| 440473133/440473133 [00:36<00:00, 12025115.90B/s]\n"
     ]
    }
   ],
   "source": [
    "max_length = 256\n",
    "model_name = 'bert-base-uncased'\n",
    "config = BertConfig.from_pretrained(model_name, output_attention=True,\n",
    "                                    output_hidden_states=True,\n",
    "                                    return_dict=True)\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "config.max_position_embeddings = max_length\n",
    "model = BertModel(config).from_pretrained(model_name)\n",
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "  \"architectures\": [\n",
       "    \"BertForMaskedLM\"\n",
       "  ],\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"finetuning_task\": null,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 768,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 3072,\n",
       "  \"layer_norm_eps\": 1e-12,\n",
       "  \"max_position_embeddings\": 512,\n",
       "  \"model_type\": \"bert\",\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"num_labels\": 2,\n",
       "  \"output_attentions\": true,\n",
       "  \"output_hidden_states\": false,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"torchscript\": false,\n",
       "  \"type_vocab_size\": 2,\n",
       "  \"vocab_size\": 30522\n",
       "}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "att_head_size = int(model.config.hidden_size/model.config.num_attention_heads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "att_head_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertEncoder(\n",
       "  (layer): ModuleList(\n",
       "    (0-11): 12 x BertLayer(\n",
       "      (attention): BertAttention(\n",
       "        (self): BertSelfAttention(\n",
       "          (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (output): BertSelfOutput(\n",
       "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (intermediate): BertIntermediate(\n",
       "        (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "      )\n",
       "      (output): BertOutput(\n",
       "        (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (LayerNorm): BertLayerNorm()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0112, -0.0324, -0.0615,  ..., -0.0383,  0.0031,  0.0059],\n",
       "        [ 0.0260, -0.0067, -0.0616,  ...,  0.1097,  0.0029, -0.0540],\n",
       "        [-0.0169,  0.0232,  0.0068,  ...,  0.0124, -0.0168,  0.0301],\n",
       "        ...,\n",
       "        [ 0.1083,  0.0056,  0.0968,  ...,  0.0188, -0.0171,  0.0141],\n",
       "        [-0.0436, -0.1032, -0.1035,  ...,  0.0138, -0.0488, -0.0453],\n",
       "        [-0.0611,  0.0224, -0.0320,  ...,  0.0376,  0.0186, -0.0482]],\n",
       "       grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.encoder.layer[0].attention.self.query.weight.T[:, 64:128]\n",
    "# head1[:, :64]  head2[:, 64:128]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "newsgroup_train = fetch_20newsgroups(subset='train')\n",
    "inputs_tests = tokenizer(newsgroup_train['data'][:1],\n",
    "                        truncation=True, padding=True, max_length=max_length,\n",
    "                        return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'token_type_ids', 'attention_mask'])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs_tests.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 201])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs_tests['input_ids'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. model output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_output = model(**inputs_tests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['attn', 'queries', 'keys'])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_output[-1][0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0053, 0.0109, 0.0052,  ..., 0.0039, 0.0036, 0.0144],\n",
       "        [0.0086, 0.0041, 0.0125,  ..., 0.0045, 0.0041, 0.0071],\n",
       "        [0.0051, 0.0043, 0.0046,  ..., 0.0043, 0.0045, 0.0031],\n",
       "        ...,\n",
       "        [0.0010, 0.0023, 0.0055,  ..., 0.0012, 0.0018, 0.0011],\n",
       "        [0.0010, 0.0023, 0.0057,  ..., 0.0012, 0.0017, 0.0007],\n",
       "        [0.0022, 0.0056, 0.0063,  ..., 0.0045, 0.0048, 0.0015]],\n",
       "       grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_output[-1][0]['attn'].shape # torch.Size([1, 12, 201, 201])\n",
    "model_output[-1][0]['attn'][0, 0, :, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_output = model.embeddings(inputs_tests['input_ids'], inputs_tests['token_type_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 201, 768])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertLayer(\n",
       "  (attention): BertAttention(\n",
       "    (self): BertSelfAttention(\n",
       "      (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (output): BertSelfOutput(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (LayerNorm): BertLayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (intermediate): BertIntermediate(\n",
       "    (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "  )\n",
       "  (output): BertOutput(\n",
       "    (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "    (LayerNorm): BertLayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.encoder.layer[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.0164,  0.0261, -0.0263,  ...,  0.0154,  0.0768,  0.0548],\n",
       "        [-0.0326,  0.0346, -0.0423,  ..., -0.0527,  0.1393,  0.0078],\n",
       "        [ 0.0105,  0.0334,  0.0109,  ..., -0.0279,  0.0258, -0.0468],\n",
       "        ...,\n",
       "        [-0.0085,  0.0514,  0.0555,  ...,  0.0282,  0.0543, -0.0541],\n",
       "        [-0.0198,  0.0944,  0.0617,  ..., -0.1042,  0.0601,  0.0470],\n",
       "        [ 0.0015, -0.0952,  0.0099,  ..., -0.0191, -0.0508, -0.0085]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.encoder.layer[0].attention.self.query.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([768])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.encoder.layer[0].attention.self.query.bias.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# emb_output[0].shape: 201*768\n",
    "# query.weight.T.shape: 768*768\n",
    "# query.weight.T[:, :att_head_size].shape: 768*64\n",
    "# -> 201*64\n",
    "q_first_head_first_layer = emb_output[0] @ model.encoder.layer[0].attention.self.query.weight.T[:,:att_head_size] \\\n",
    "                        + model.encoder.layer[0].attention.self.query.bias[:att_head_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([201, 64])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_first_head_first_layer.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_first_head_first_layer = emb_output[0] @ model.encoder.layer[0].attention.self.key.weight.T[:,:att_head_size] \\\n",
    "                        + model.encoder.layer[0].attention.self.key.bias[:att_head_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([201, 64])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k_first_head_first_layer.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_scores = torch.nn.Softmax(q_first_head_first_layer @ k_first_head_first_layer.T / math.sqrt(att_head_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_scores = torch.nn.Softmax(dim=-1)(q_first_head_first_layer @ k_first_head_first_layer.T / math.sqrt(att_head_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0053, 0.0109, 0.0052,  ..., 0.0039, 0.0036, 0.0144],\n",
       "        [0.0086, 0.0041, 0.0125,  ..., 0.0045, 0.0041, 0.0071],\n",
       "        [0.0051, 0.0043, 0.0046,  ..., 0.0043, 0.0045, 0.0031],\n",
       "        ...,\n",
       "        [0.0010, 0.0023, 0.0055,  ..., 0.0012, 0.0018, 0.0011],\n",
       "        [0.0010, 0.0023, 0.0057,  ..., 0.0012, 0.0017, 0.0007],\n",
       "        [0.0022, 0.0056, 0.0063,  ..., 0.0045, 0.0048, 0.0015]],\n",
       "       grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_scores # (201*64) * (64*201) -> (201*201)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_first_head_first_layer = emb_output[0] @ model.encoder.layer[0].attention.self.value.weight.T[:,:att_head_size] \\\n",
    "                        + model.encoder.layer[0].attention.self.value.bias[:att_head_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([201, 64])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v_first_head_first_layer.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_emb = attn_scores @ v_first_head_first_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-4.5640e-01,  4.6211e-02,  4.3913e-02,  ..., -2.0099e-02,\n",
       "         -1.2756e-02,  6.4255e-03],\n",
       "        [-4.5674e-01,  3.4322e-02,  3.2707e-02,  ..., -4.9206e-02,\n",
       "          1.4975e-02, -3.0628e-02],\n",
       "        [-4.9474e-01, -2.9539e-04, -7.5375e-04,  ..., -2.0035e-02,\n",
       "          1.7146e-02, -3.0126e-02],\n",
       "        ...,\n",
       "        [-3.7991e-01,  5.2831e-02,  2.2534e-02,  ..., -1.8338e-02,\n",
       "         -6.9508e-02,  2.1317e-02],\n",
       "        [-3.8071e-01,  4.0900e-02,  2.8770e-02,  ..., -2.1192e-02,\n",
       "         -5.2893e-02,  1.9734e-02],\n",
       "        [-4.7131e-01,  1.0947e-01,  1.1631e-02,  ..., -3.4542e-02,\n",
       "         -2.3752e-02, -5.0505e-03]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([201, 64])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
