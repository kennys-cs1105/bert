{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from transformers.models.bert import BertModel, BertTokenizer, BertForMaskedLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. model load and data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_type = 'bert-base-uncased'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file vocab.txt from cache at C:\\Users\\kennyS\\.cache\\huggingface\\hub\\models--bert-base-uncased\\snapshots\\1dbc166cf8765166998eff31ade2eb64c8a40076\\vocab.txt\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at C:\\Users\\kennyS\\.cache\\huggingface\\hub\\models--bert-base-uncased\\snapshots\\1dbc166cf8765166998eff31ade2eb64c8a40076\\tokenizer_config.json\n",
      "loading file tokenizer.json from cache at C:\\Users\\kennyS\\.cache\\huggingface\\hub\\models--bert-base-uncased\\snapshots\\1dbc166cf8765166998eff31ade2eb64c8a40076\\tokenizer.json\n",
      "loading configuration file config.json from cache at C:\\Users\\kennyS\\.cache\\huggingface\\hub\\models--bert-base-uncased\\snapshots\\1dbc166cf8765166998eff31ade2eb64c8a40076\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.37.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at C:\\Users\\kennyS\\.cache\\huggingface\\hub\\models--bert-base-uncased\\snapshots\\1dbc166cf8765166998eff31ade2eb64c8a40076\\config.json\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.37.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file model.safetensors from cache at C:\\Users\\kennyS\\.cache\\huggingface\\hub\\models--bert-base-uncased\\snapshots\\1dbc166cf8765166998eff31ade2eb64c8a40076\\model.safetensors\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n",
      "loading configuration file config.json from cache at C:\\Users\\kennyS\\.cache\\huggingface\\hub\\models--bert-base-uncased\\snapshots\\1dbc166cf8765166998eff31ade2eb64c8a40076\\config.json\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_hidden_states\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.37.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file model.safetensors from cache at C:\\Users\\kennyS\\.cache\\huggingface\\hub\\models--bert-base-uncased\\snapshots\\1dbc166cf8765166998eff31ade2eb64c8a40076\\model.safetensors\n",
      "Generate config GenerationConfig {\n",
      "  \"output_hidden_states\": true,\n",
      "  \"pad_token_id\": 0\n",
      "}\n",
      "\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of BertForMaskedLM were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForMaskedLM for predictions without further training.\n",
      "Generation config file not found, using a generation config created from the model config.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(model_type)\n",
    "bert = BertModel.from_pretrained(model_type)\n",
    "mlm = BertForMaskedLM.from_pretrained(model_type, output_hidden_states=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = (\"After Abraham Lincoln won the November 1860 presidential\"\n",
    "        \"election on an anti-slavery platform, an initial seven\"\n",
    "        \"slave states declared their secession from the country \"\n",
    "        \"to form the Confederacy. War broke out in April 1861 \"\n",
    "        \"when secesstionist forces attacked Fort Sumter in South \"\n",
    "        \"Carolina, just over a month after Lincoln's inauguration.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"After Abraham Lincoln won the November 1860 presidentialelection on an anti-slavery platform, an initial sevenslave states declared their secession from the country to form the Confederacy. War broke out in April 1861 when secesstionist forces attacked Fort Sumter in South Carolina, just over a month after Lincoln's inauguration.\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(text, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,  2044,  8181,  5367,  2180,  1996,  2281,  7313,  4883, 12260,\n",
       "          7542,  2006,  2019,  3424,  1011,  8864,  4132,  1010,  2019,  3988,\n",
       "         19463, 14973,  2063,  2163,  4161,  2037, 22965,  2013,  1996,  2406,\n",
       "          2000,  2433,  1996, 18179,  1012,  2162,  3631,  2041,  1999,  2258,\n",
       "          6863,  2043, 10819,  7971,  3508,  2923,  2749,  4457,  3481,  7680,\n",
       "          3334,  1999,  2148,  3792,  1010,  2074,  2058,  1037,  3204,  2044,\n",
       "          5367,  1005,  1055, 17331,  1012,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 66])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs['input_ids'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs['labels'] = inputs['input_ids'].detach().clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  101,  2044,  8181,  5367,  2180,  1996,  2281,  7313,  4883, 12260,\n",
       "          7542,  2006,  2019,  3424,  1011,  8864,  4132,  1010,  2019,  3988,\n",
       "         19463, 14973,  2063,  2163,  4161,  2037, 22965,  2013,  1996,  2406,\n",
       "          2000,  2433,  1996, 18179,  1012,  2162,  3631,  2041,  1999,  2258,\n",
       "          6863,  2043, 10819,  7971,  3508,  2923,  2749,  4457,  3481,  7680,\n",
       "          3334,  1999,  2148,  3792,  1010,  2074,  2058,  1037,  3204,  2044,\n",
       "          5367,  1005,  1055, 17331,  1012,   102]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = torch.rand(inputs['input_ids'].shape) < 0.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False, False, False, False, False, False, False, False,  True, False,\n",
       "         False, False, False,  True, False, False, False,  True, False,  True,\n",
       "         False, False,  True, False, False, False,  True, False,  True, False,\n",
       "         False, False, False, False,  True, False,  True, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False,\n",
       "          True, False,  True, False, False, False]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(11)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(mask[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.16666666666666666"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "11/66"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_arr = (torch.rand(inputs['input_ids'].shape) < 0.15) \\\n",
    "    * (inputs['input_ids'] != 101) \\\n",
    "    * (inputs['input_ids'] != 102)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False,  True, False, False,\n",
       "         False, False, False, False, False, False, False, False,  True, False,\n",
       "         False, False, False, False, False,  True, False, False, False,  True,\n",
       "         False, False, False, False, False, False, False, False, False, False,\n",
       "         False,  True, False, False, False, False,  True, False, False, False,\n",
       "         False,  True, False, False, False, False]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(7)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(mask_arr[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "selection = torch.flatten(mask_arr[0].nonzero()).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[17, 28, 35, 39, 51, 56, 61]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs['input_ids'][0, selection] = 103"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,  2044,  8181,  5367,  2180,  1996,  2281,  7313,  4883, 12260,\n",
       "          7542,  2006,  2019,  3424,  1011,  8864,  4132,   103,  2019,  3988,\n",
       "         19463, 14973,  2063,  2163,  4161,  2037, 22965,  2013,   103,  2406,\n",
       "          2000,  2433,  1996, 18179,  1012,   103,  3631,  2041,  1999,   103,\n",
       "          6863,  2043, 10819,  7971,  3508,  2923,  2749,  4457,  3481,  7680,\n",
       "          3334,   103,  2148,  3792,  1010,  2074,   103,  1037,  3204,  2044,\n",
       "          5367,   103,  1055, 17331,  1012,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'labels': tensor([[  101,  2044,  8181,  5367,  2180,  1996,  2281,  7313,  4883, 12260,\n",
       "          7542,  2006,  2019,  3424,  1011,  8864,  4132,  1010,  2019,  3988,\n",
       "         19463, 14973,  2063,  2163,  4161,  2037, 22965,  2013,  1996,  2406,\n",
       "          2000,  2433,  1996, 18179,  1012,  2162,  3631,  2041,  1999,  2258,\n",
       "          6863,  2043, 10819,  7971,  3508,  2923,  2749,  4457,  3481,  7680,\n",
       "          3334,  1999,  2148,  3792,  1010,  2074,  2058,  1037,  3204,  2044,\n",
       "          5367,  1005,  1055, 17331,  1012,   102]])}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. forward and calculate loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlm.eval()\n",
    "with torch.no_grad():\n",
    "    output = mlm(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['loss', 'logits', 'hidden_states'])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ -7.0729,  -6.9823,  -7.0373,  ...,  -6.2476,  -6.2185,  -4.2436],\n",
       "         [-12.4250, -12.2744, -12.3999,  ..., -11.5897, -11.1210,  -9.2143],\n",
       "         [ -6.0864,  -6.2552,  -5.7428,  ...,  -6.1365,  -5.8874,  -4.5088],\n",
       "         ...,\n",
       "         [ -3.9977,  -4.0382,  -3.9518,  ...,  -2.9818,  -2.6752,  -8.1843],\n",
       "         [-14.2224, -14.0590, -14.0979,  ..., -11.1725, -11.1214,  -9.1664],\n",
       "         [-10.4361, -10.7536, -10.2923,  ...,  -9.9935,  -7.9635,  -7.9996]]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.7957)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(output['hidden_states'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "ce = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([66])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs['labels'][0].view(-1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_hidden_state = output['hidden_states'][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ -7.0729,  -6.9823,  -7.0373,  ...,  -6.2476,  -6.2185,  -4.2436],\n",
       "         [-12.4250, -12.2744, -12.3999,  ..., -11.5897, -11.1210,  -9.2143],\n",
       "         [ -6.0864,  -6.2552,  -5.7428,  ...,  -6.1365,  -5.8874,  -4.5088],\n",
       "         ...,\n",
       "         [ -3.9977,  -4.0382,  -3.9518,  ...,  -2.9818,  -2.6752,  -8.1843],\n",
       "         [-14.2224, -14.0590, -14.0979,  ..., -11.1725, -11.1214,  -9.1664],\n",
       "         [-10.4361, -10.7536, -10.2923,  ...,  -9.9935,  -7.9635,  -7.9996]]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlm.eval()\n",
    "with torch.no_grad():\n",
    "    transformed = mlm.cls.predictions.transform(last_hidden_state)\n",
    "    logits = mlm.cls.predictions.decoder(transformed)\n",
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.7957)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ce(logits[0], inputs['labels'][0].view(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1012,  2044,  8181,  5367,  2180,  1996,  2281,  7313,  4883,  2624,\n",
       "         7542,  2006,  2019,  3424,  1011,  8864,  4132,  1010,  2019,  3988,\n",
       "        19463, 14973,  2063,  2163,  4161,  2037, 22965,  2013,  1996,  2406,\n",
       "         2000,  2433,  1996, 18179,  1012,  4808, 12591,  2162,  1999,  2285,\n",
       "         6863,  2043, 10819,  7971,  3508,  2923,  2749,  4457,  3481,  7680,\n",
       "        10907,  1999,  2148,  3792,  1010,  2074,  2058,  1037,  3204,  2044,\n",
       "         5367,  1005,  1055, 17331,  1012,  1055])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.argmax(logits[0], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'BertTokenizer' object has no attribute 'convert_ids_to_token'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[39], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_ids_to_token\u001b[49m(inputs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m]))\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'BertTokenizer' object has no attribute 'convert_ids_to_token'"
     ]
    }
   ],
   "source": [
    "' '.join(tokenizer.convert_ids_to_token(inputs['input_ids'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
